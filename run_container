#!/usr/bin/env bash

# --- Configuration ---
# Image Definitions
COFFEA_IMAGE="docker://gitlab-registry.cern.ch/cms-cmu/barista:latest"
COMBINE_IMAGE="docker://gitlab-registry.cern.ch/cms-analysis/general/combine-container:CMSSW_11_3_4-combine_v9.1.0-harvester_v2.1.0"
SNAKEMAKE_IMAGE="docker://gitlab-registry.cern.ch/cms-cmu/barista:reana_latest"   ### depracated, use local Pixi environment instead
#CLASSIFIER_IMAGE="docker://chuyuanliu/heptools:ml"
CLASSIFIER_IMAGE="docker://gitlab-registry.cern.ch/cms-cmu/barista:classifier_latest"
CLASSIFIER_CPU_IMAGE="docker://gitlab-registry.cern.ch/cms-cmu/barista:classifier_cpu_latest"
BRILCALC_IMAGE="docker://gitlab-registry.cern.ch/cms-cloud/brilws-docker:latest"

# Resolve CVMFS paths if available
if [ -d '/cvmfs/unpacked.cern.ch' ]; then
    COFFEA_IMAGE="/cvmfs/unpacked.cern.ch/${COFFEA_IMAGE#docker://}"
    COMBINE_IMAGE="/cvmfs/unpacked.cern.ch/${COMBINE_IMAGE#docker://}"
    SNAKEMAKE_IMAGE="/cvmfs/unpacked.cern.ch/${SNAKEMAKE_IMAGE#docker://}"
    CLASSIFIER_IMAGE="/cvmfs/unpacked.cern.ch/${CLASSIFIER_IMAGE#docker://}"
    CLASSIFIER_CPU_IMAGE="/cvmfs/unpacked.cern.ch/${CLASSIFIER_CPU_IMAGE#docker://}"
    BRILCALC_IMAGE="/cvmfs/unpacked.cern.ch/${BRILCALC_IMAGE#docker://}"
fi

# --- Helpers ---
info() { echo -e "\033[1;34m[INFO]\033[0m $1"; }
error() { echo -e "\033[1;31m[ERROR]\033[0m $1"; }
warning() { echo -e "\033[1;33m[WARNING]\033[0m $1"; }

check_apptainer() {
    command -v apptainer >/dev/null 2>&1 || { error "Apptainer is not installed. Aborting."; exit 1; }
}

show_help() {
    cat << EOF
Usage: $0 [command] [options]

Commands:
  [command...]          Run commands inside the coffea container.
                        (Interactive shell is the only option to run on LPC HTCondor).
  combine [cmd...]      Run commands inside the combine container.
  classifier [cmd...]   Run commands inside the classifier with GPU container.
  classifier_cpu [cmd...] Run commands inside the classifier with CPU container.
  brilcalc [cmd...]     Run commands inside the brilcalc container.
  snakemake [opts]      Run snakemake using the local Pixi environment.
                        Use --reinstall to clean environment.
  snakemake_container   Run snakemake inside a container (backup option).
  --help                Show this help message.

Examples:
  $0                     Open interactive shell (Coffea).
  $0 python script.py    Run script in Coffea container.
  $0 combine             Open Combine container.
  $0 snakemake -s Snakefile
  $0 classifier          Run commands inside the classifier with GPU container.
  $0 classifier_cpu      Run commands inside the classifier with CPU container.
EOF
}

# --- Host Detection & Setup ---
PIXI_DIR="$HOME/.pixi"
setup_host_env() {
    local HOST
    HOST=$(hostname)
    info "Determining configuration for hostname: $HOST"

    # Common Bind Paths
    local GRID_BINDS="/cvmfs/grid.cern.ch/etc/grid-security:/etc/grid-security"
    local COMMON_BINDS="/cvmfs,${GRID_BINDS}"
    
    # Standard Cache Setup (using /tmp to be safe/fast)
    local CACHE_BASE="/tmp/$(whoami)"
    export USE_SLURM=false

    case "$HOST" in
        *cmslpc*)
            export APPTAINER_BINDPATH="${COMMON_BINDS},/uscmst1b_scratch,/uscms_data"
            PIXI_DIR="/uscms_data/d3/${USER}/.pixi"
            if [ ! -d "$PIXI_DIR" ]; then
                info "LPC environment detected. Pixi will be installed in ${PIXI_DIR}"
            fi
            ;;
        *lxplus*)
            export APPTAINER_BINDPATH="${COMMON_BINDS},/afs,/eos"
            ;;
        *falcon*|*rogue*)
            # 1. Basic Mounts
            if [[ $HOST == *falcon* ]]; then
                export APPTAINER_BINDPATH="${COMMON_BINDS},/home/export/"
            else
                export APPTAINER_BINDPATH="${COMMON_BINDS},/home/export/,/mnt/scratch/${USER}"
            fi
            
            # 2. SLURM CONFIG, USERS, & MUNGE (CRITICAL FIX)
            #    Added /run/munge and /var/run/munge so sbatch can authenticate
            export APPTAINER_BINDPATH="${APPTAINER_BINDPATH},/etc/slurm,/etc/passwd,/etc/group,/run/munge,/var/run/munge"

            # 3. QUARANTINE BINDS
            export APPTAINER_BINDPATH="${APPTAINER_BINDPATH},/usr:/host/usr,/lib:/host/lib,/lib64:/host/lib64"

            # 4. PLUGIN BIND
            export APPTAINER_BINDPATH="${APPTAINER_BINDPATH},/usr/lib/x86_64-linux-gnu/slurm-wlm"

            # 5. GENERATE WRAPPER
            WRAPPER_DIR="${CACHE_BASE}/slurm_wrappers"
            mkdir -p "$WRAPPER_DIR"
            
            cat <<EOF > "${WRAPPER_DIR}/slurm_wrapper.sh"
#!/bin/bash
# Define Host Library Paths (Ubuntu/Debian specific)
HOST_LIBS="/host/lib/x86_64-linux-gnu:/host/usr/lib/x86_64-linux-gnu:/host/usr/lib/x86_64-linux-gnu/slurm-wlm:/host/lib64:/host/usr/lib64"

# Define Host Loader
if [ -f "/host/lib64/ld-linux-x86-64.so.2" ]; then
    LOADER="/host/lib64/ld-linux-x86-64.so.2"
else
    LOADER="/host/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2"
fi

# Target Binary
NAME=\$(basename "\$0")
BINARY="/host/usr/bin/\$NAME"

# Execute
exec "\$LOADER" --library-path "\$HOST_LIBS" "\$BINARY" "\$@"
EOF
            chmod +x "${WRAPPER_DIR}/slurm_wrapper.sh"

            # Create Symlinks
            for cmd in sbatch squeue scancel scontrol; do
                ln -sf "${WRAPPER_DIR}/slurm_wrapper.sh" "${WRAPPER_DIR}/${cmd}"
            done

            # 6. Add wrappers to PATH (Must be FIRST)
            export APPTAINER_BINDPATH="${APPTAINER_BINDPATH},${WRAPPER_DIR}:/opt/slurm_wrappers"
            export APPTAINERENV_PATH="/opt/slurm_wrappers:\${PATH}"
            export SINGULARITYENV_PATH="/opt/slurm_wrappers:\${PATH}"
            
            # 7. Enable Slurm Mode if we aren't already in a job
            if [ -z "$SLURM_JOB_ID" ]; then
                export USE_SLURM=true 
            fi
            ;;
        *)
            warning "Unknown hostname. Using defaults (HOME) for cache/config."
            CACHE_BASE=$HOME
            mkdir -p "$CACHE_BASE"
            ;;
    esac

    # Cache Configuration
    export APPTAINER_CACHEDIR="${CACHE_BASE}/apptainer_cache"
    export APPTAINER_TMPDIR="${CACHE_BASE}/.apptainer/"
    export MPLCONFIGDIR="${CACHE_BASE}/.config/matplotlib"
    mkdir -p "$APPTAINER_CACHEDIR" "$APPTAINER_TMPDIR" "$MPLCONFIGDIR"
}

# --- LPC Specific Logic ---
setup_lpc_jobqueue() {
    # Only run on LPC
    [[ $(hostname) != *"cmslpc"* ]] && return 0

    # Bootstrap if missing
    if [[ ! -f ".bashrc" || ! -f ".cmslpc-local-conf" ]]; then
        info "Setting up lpcjobqueue configuration..."
        curl -OL https://raw.githubusercontent.com/CoffeaTeam/lpcjobqueue/main/bootstrap.sh
        bash bootstrap.sh
        rm bootstrap.sh shell
    else
        info "lpcjobqueue configuration found. Skipping bootstrap."
    fi

    # Define LPC specific variables and binds
    local LPC_CONDOR_CONFIG="/etc/condor/config.d/01_cmslpc_interactive"
    local LPC_CONDOR_LOCAL="/usr/local/bin/cmslpc-local-conf.py"
    
    export APPTAINER_BINDPATH="${APPTAINER_BINDPATH},${LPC_CONDOR_CONFIG},${LPC_CONDOR_LOCAL}:${LPC_CONDOR_LOCAL}.orig,${PWD}/.cmslpc-local-conf:${LPC_CONDOR_LOCAL}"
}

# --- Container Functions ---

run_coffea() {
    check_apptainer
    info "Running Coffea container..."
    setup_lpc_jobqueue

    # 1. Base Flags
    local FLAGS=( -B "$PWD":/srv --pwd /srv/ )
    
    # LPC-Specific Environment Variable
    if [[ $(hostname) == *"cmslpc"* ]]; then
        FLAGS+=( --env COFFEA_IMAGE_FULL="$COFFEA_IMAGE" )
    fi

    # 2. FALCON / ROGUE Logic
    if [[ $(hostname) == *falcon* || $(hostname) == *rogue* ]]; then
        # Critical: Mirror Bind for Dask
        FLAGS=( -B "$PWD":"$PWD" --pwd "$PWD" )

        if [ -n "$SLURM_JOB_ID" ]; then
            # CASE A: Inside a job -> Run locally
            info "Inside Slurm Job ($SLURM_JOB_ID). Running container directly..."
        else
            # CASE B: Login Node -> Submit Interactive Job
            info "Not in a job. Launching interactive Slurm session..."
            srun --pty --partition=work --time=04:00:00 --mem=4G "$0" "$@"
            return # Stop execution on login node
        fi
    fi

    # 3. EXECUTION LOGIC
    if [ -z "$*" ]; then
        # --- INTERACTIVE SHELL ---
        if [[ $(hostname) == *"cmslpc"* ]]; then
            info "Opening interactive shell (LPC mode)..."
            APPTAINER_SHELL=$(which bash) apptainer exec "${FLAGS[@]}" "$COFFEA_IMAGE" /bin/bash --rcfile /srv/.bashrc
        else
            info "Opening interactive shell..."
            APPTAINER_SHELL=$(which bash) apptainer exec "${FLAGS[@]}" "$COFFEA_IMAGE" /bin/bash
        fi
    else
        # --- COMMAND EXECUTION ---
        local CMD="$*"
        if [[ $(hostname) == *"cmslpc"* ]]; then
            info "Running command (LPC mode)..."
            APPTAINER_SHELL=$(which bash) apptainer exec "${FLAGS[@]}" "$COFFEA_IMAGE" /bin/bash -c "source /srv/.bashrc && $CMD"
        else
            info "Running command..."
            APPTAINER_SHELL=$(which bash) apptainer exec "${FLAGS[@]}" "$COFFEA_IMAGE" /bin/bash -c "$CMD"
        fi
    fi
}

run_combine() {
    check_apptainer
    info "Running Combine container..."
    local SETUP="source /cvmfs/cms.cern.ch/cmsset_default.sh && cd /home/cmsusr/CMSSW_11_3_4/ && cmsenv && cd /home/cmsusr/barista/"
    local FLAGS=( -B "${PWD}:/home/cmsusr/barista/" --pwd /home/cmsusr/barista/ )

    if [ -z "$*" ]; then
        APPTAINER_SHELL=$(which bash) apptainer exec "${FLAGS[@]}" "${COMBINE_IMAGE}" /bin/bash -i -c "$SETUP && bash"
    else
        APPTAINER_SHELL=$(which bash) apptainer exec "${FLAGS[@]}" "${COMBINE_IMAGE}" /bin/bash -c "$SETUP && $*"
    fi
}

run_brilcalc() {
    check_apptainer
    info "Running Brilcalc container..."
    local FLAGS=( -B "${PWD}:/srv" --pwd /srv/ --env PYTHONPATH=/home/bril/.local/lib/python3.10/site-packages )

    if [ -z "$*" ]; then
        APPTAINER_SHELL=$(which bash) apptainer exec "${FLAGS[@]}" "${BRILCALC_IMAGE}" /bin/bash
    else
        APPTAINER_SHELL=$(which bash) apptainer exec "${FLAGS[@]}" "${BRILCALC_IMAGE}" /bin/bash -c "$*"
    fi
}

run_classifier() {
    check_apptainer
    info "Running Classifier container..."

    local INIT_FILE="/entrypoint.sh"
    local USE_SLURM=false
    local FLAGS=( -B "$PWD":/srv --nv --pwd /srv )
    ulimit -n 65536

    # Bind X509 proxy for XRootD authentication
    local PROXY="/tmp/x509up_u$(id -u)"
    if [ -f "$PROXY" ]; then
        FLAGS+=( -B "$PROXY":"$PROXY" --env X509_USER_PROXY="$PROXY" )
    fi
    
    # Parse out "slurm" and "cpu" flags from args
    for arg in "$@"; do
        if [[ "$arg" == "slurm" ]]; then
            USE_SLURM=true
        elif [[ "$arg" == "cpu" ]]; then
            info "Using CPU version of the classifier container..."
            IMAGE="$CLASSIFIER_CPU_IMAGE"
        else
            ARGS+=("$arg")
        fi
    done
    local USER_CMD="${ARGS[*]}"
    
    if [[ "$1" == "cpu" ]]; then
        info "Using CPU version of the classifier container..."
        IMAGE="$CLASSIFIER_CPU_IMAGE"
    else
        info "Using GPU version of the classifier container..."
        IMAGE="$CLASSIFIER_IMAGE"
    fi
    
    if [ "$USE_SLURM" = true ]; then
        local CMD="source $INIT_FILE && export PYTHONUNBUFFERED=1 && $USER_CMD"
        info "Submitting batch job to Slurm..."
        sbatch <<-EOT
		#!/bin/bash
		#SBATCH --job-name=classifier_batch
		#SBATCH --output=logs/%x_%j.out
		#SBATCH --nodes=1
		#SBATCH --cpus-per-task=4
		#SBATCH --mem=16G
		#SBATCH --gres=gpu:1
		#SBATCH --time=04:00:00
		apptainer exec ${FLAGS[@]} "$IMAGE" bash -c "$CMD"
		EOT
    elif [ -z "$USER_CMD" ]; then
        APPTAINER_SHELL=$(which bash) apptainer exec "${FLAGS[@]}" "$IMAGE" bash --init-file "$INIT_FILE" # interactive shell
    else
        APPTAINER_SHELL=$(which bash) apptainer exec "${FLAGS[@]}" "$IMAGE" bash --init-file "$INIT_FILE" -c "$USER_CMD" # user command locally
    fi
}

run_snakemake_container() {
    check_apptainer
    info "Running Snakemake container..."
    [[ -z "$1" ]] && { error "No arguments supplied."; show_help; exit 1; }
    [[ "$*" != *"--snakefile"* && "$*" != *"-s"* ]] && { error "--snakefile (-s) argument required."; exit 1; }

    APPTAINER_SHELL=$(which bash) apptainer exec -B "$PWD":/srv --pwd /srv/ "$SNAKEMAKE_IMAGE" /bin/bash -c "snakemake $*"
}

# --- Pixi / Snakemake Local ---

install_pixi() {
    if ! command -v pixi &> /dev/null; then
        info "Installing Pixi to ${PIXI_DIR}..."
        export PIXI_HOME="${PIXI_DIR}"
        mkdir -p "${PIXI_DIR}"
        curl -fsSL https://pixi.sh/install.sh | bash || return 1
        export PATH="${PIXI_DIR}/bin:$PATH"
    fi
    command -v pixi &> /dev/null || return 1
}

run_snakemake() {
    info "Running Snakemake with Pixi environment..."
    set -eo pipefail

    local REINSTALL=false
    local ARGS=()
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --reinstall|--fresh) REINSTALL=true; shift ;;
            --help|-h) ARGS+=("--help"); shift ;;
            source) ARGS+=("$@"); break ;;
            *) ARGS+=("$1"); shift ;;
        esac
    done

    [[ ${#ARGS[@]} -eq 0 && "$REINSTALL" != true ]] && { error "No arguments supplied."; show_help; exit 1; }

    # Install Pixi
    install_pixi || { error "Pixi installation failed."; exit 1; }

    # Reinstall logic
    if [ "$REINSTALL" = true ]; then
        info "Cleaning up environment..."
        rm -f pixi.toml pixi.lock
    fi

    # Sync Config
    local PIXI_CONFIG="software/pixi/pixi.toml"
    if [ -f "${PIXI_CONFIG}" ]; then
        cp "${PIXI_CONFIG}" pixi.toml
    else
        error "Template ${PIXI_CONFIG} not found."; exit 1
    fi

    # Install Environment
    pixi install || { error "Pixi install failed."; exit 1; }

    # Install Reana Client (Custom)
    if ! pixi run python -c "import reana_client" 2>/dev/null; then
        info "Installing reana-client..."
        pixi run install-reana || warning "reana-client install failed."
    fi

    # Execute
    if [ "${ARGS[0]}" = "source" ]; then
        pixi run bash -c "${ARGS[*]}"
    else
        pixi run snakemake "${ARGS[@]}"
    fi
    info "Execution finished successfully!"
}

# --- Main Dispatch ---

setup_host_env

if [ "$1" == "--help" ]; then
    show_help
    exit 0
fi

case "$1" in
    "combine")             run_combine "${@:2}" ;;
    "brilcalc")            run_brilcalc "${@:2}" ;;
    "snakemake_container") run_snakemake_container "${@:2}" ;;
    "snakemake")           run_snakemake "${@:2}" ;;
    "classifier")          run_classifier "${@:2}" ;;
    "classifier_cpu")      run_classifier "cpu" "${@:2}" ;;
    *)                     run_coffea "$@" ;;
esac
